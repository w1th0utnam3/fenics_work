import sys
import cffi
import time
import argparse
import importlib
import numpy as np
from typing import Dict, List, Tuple

from ttbench.types import BenchmarkReport, TestCase, TestRunParameters, FormTestData, FormTestResult


def parse_args(args):
    parser = argparse.ArgumentParser(prog="{} run".format(sys.argv[0]),
                                     description="Runs previously generated benchmark data. Does not require FEniCS.")
    parser.add_argument("data_filename", help="Input filename for the benchmark data that should be run.")
    parser.add_argument("report_filename", help="Output filename for the benchmark report that should be generated.")

    return parser.parse_args(args)


def compile_cffi(module_name: str,
                 code_c: str,
                 code_h: str,
                 define_macros: List[Tuple[str, str]] = None,
                 compiler_args: List[str] = None,
                 verbose: bool = False) -> str:
    """Compiles a module from the specified source code using CFFI."""

    ffi = cffi.FFI()
    ffi.set_source(module_name, code_c, define_macros=define_macros, extra_compile_args=compiler_args)
    ffi.cdef(code_h)
    lib = ffi.compile(verbose=verbose)

    return lib


def import_cffi(module_name: str):
    """Imports a module generated by CFFI."""

    mod = importlib.import_module(module_name)
    ffi, lib = mod.ffi, mod.lib
    return ffi, lib


def timing(n_runs: int, func, warm_up: bool = True, verbose: bool = True, name: str = None) -> Tuple[
    float, float, float, List]:
    """Measures avg, min and max execution time of 'func' over 'n_runs' executions"""

    if name is None:
        name = str(func)

    lower = float('inf')
    upper = -float('inf')
    avg = 0

    results = []

    if verbose:
        print(f"Timing (runs:{n_runs}): '{name}' - ", end="", flush=True)

    # Call once without measurement "to get warm"
    if warm_up:
        if verbose:
            print("warm-up...", end="", flush=True)

        results.append(func())

        if verbose:
            print("done. ", end="", flush=True)

    for i in range(n_runs):
        start = time.time()
        results.append(func())
        end = time.time()

        diff = end - start

        lower = min(lower, diff)
        upper = max(upper, diff)
        avg += (diff - avg) / (i + 1)

        if verbose:
            print("#", end="", flush=True)

    if verbose:
        print(" done.")

    return avg, lower, upper, results


def run_benchmark(test_case: TestCase, test_fun_names: Dict, codes: List[Tuple[str, str]],
                  verbose: bool = True) -> BenchmarkReport:
    """
    Runs a benchmark TestCase and generates a corresponding BenchmarkReport.

    :param test_case: The TestCase to process.
    :param test_fun_names: A dict containing the C function names of the test wrappers, from generate_benchmark_code.
    :param code_c: The C code of the test case.
    :param code_h: The C header of the test case.
    :return: Benchmark report: dict over all form names, containing dicts (i,j) -> FormTestResult, where (i,j) are
        combinations of compiler parameters and form compiler parameters.
    """

    raw_results = {form_def.form_name: dict() for form_def in test_case.forms}

    module_cache = dict()
    module_counter = 0

    def compile_module(index: int, args, macros=None):
        nonlocal module_counter

        if index in module_cache.keys():
            return module_cache[index]
        else:
            compile_cffi("_benchmark_{}".format(module_counter), codes[index][0], codes[index][1],
                         define_macros=macros,
                         compiler_args=args, verbose=verbose)
            ffi, lib = import_cffi("_benchmark_{}".format(module_counter))

            module_cache[index] = ffi, lib
            module_counter += 1

            return ffi, lib

    # Step 1: Run benchmark
    # ---

    t_start = time.time()

    total_tests = len(test_case.compiler_args) * len(test_case.run_args) * len(test_case.forms)
    test_counter = 1
    # Outermost loop over all compiler argument sets
    for i, compiler_arg_set in enumerate(test_case.compiler_args):
        active_compile_args = [arg for arg, use in compiler_arg_set.items() if use]

        # Loop over the forms
        for form_idx, (form_name, form_fun_names) in enumerate(test_fun_names.items()):
            # Loop over the function names of each test case
            for j, (fun_name, fun_index) in enumerate(form_fun_names):
                if verbose:
                    print("({}/{}) Running test function '{}'".format(test_counter, total_tests, fun_name))

                run_arg_set = test_case.run_args[j]  # type: TestRunParameters
                form_data = test_case.forms[form_idx]  # type: FormTestData

                n_elem = form_data.n_elems
                w = form_data.coefficients
                coords_dof = form_data.coord_dofs

                # If cross element vectorization is enabled, tile coefficient and dof arrays accordingly
                cross_element_width = run_arg_set.cross_element_width
                if cross_element_width > 0:
                    n_elem = int(n_elem / cross_element_width)

                    w = np.tile(np.expand_dims(w, 2), cross_element_width)
                    coords_dof = np.tile(np.expand_dims(coords_dof, 2), cross_element_width)

                # Get or compile the test module
                ffi, lib = compile_module(fun_index, args=active_compile_args, macros=test_case.compiler_defines)

                # Get pointers to numpy arrays
                w_ptr = ffi.cast("double*", w.ctypes.data)
                coords_dof_ptr = ffi.cast("double*", coords_dof.ctypes.data)

                # Get the function that should be called
                fun = getattr(lib, fun_name)

                # How many times the test should be run
                n_runs = test_case.n_repeats
                # The actual test callable
                test_callable = lambda: fun(n_elem, w_ptr, coords_dof_ptr)

                # Run the timing
                avg, min, max, return_vals = timing(n_runs, test_callable, verbose=verbose, name=fun_name)

                # Store result
                raw_results[form_name][(i, j)] = avg, min, max, return_vals[0]

                test_counter += 1
                if verbose:
                    print("")

        # Clear compile cache
        module_cache.clear()

    t_end = time.time()

    # Step 2: Generate report and calculate speedup)
    # ---

    # Dict for results that will be returned (includes speedup)
    results = {form_def.form_name: dict() for form_def in test_case.forms}

    # Loop over results of all forms
    for form_idx, (form_name, form_results) in enumerate(raw_results.items()):
        # Obtain the reference time for the form
        reference_time = form_results[(test_case.reference_case)][0]
        reference_result = form_results[(test_case.reference_case)][3]

        for i, compiler_arg_set in enumerate(test_case.compiler_args):
            for j, run_arg_set in enumerate(test_case.run_args):
                raw_result = form_results[(i, j)]
                speedup = reference_time / raw_result[0]
                if (i, j) == test_case.reference_case:
                    result_ok = "Reference"
                else:
                    result_ok = np.allclose(reference_result, raw_result[3], rtol=1e-7, atol=1e-10)

                # Store the result values including speedup
                results[form_name][(i, j)] = FormTestResult(avg=raw_result[0], min=raw_result[1], max=raw_result[2],
                                                            speedup=speedup, result_val=raw_result[3],
                                                            result_ok=result_ok)

    runtime = t_end - t_start
    return BenchmarkReport(runtime, results)
