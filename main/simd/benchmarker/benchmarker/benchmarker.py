import ufl
import ffc.compiler
import ffc.parameters

import cffi
import importlib
import numpy as np
from copy import copy
from typing import Dict, List, Tuple

from benchmarker.types import FormTestData, FormTestResult, TestCase, TestRunArgs
from benchmarker.c_code import wrap_tabulate_tensor_code, join_test_wrappers
import benchmarker.forms as forms
import simd.utils as utils


def compile_form(form: ufl.Form, prefix: str, extra_ffc_args=None) -> Tuple[str, str]:
    """Compiles an UFL form and returns the source code of the tabulate_tensor funciton"""

    prefix = prefix.lower()
    parameters = ffc.parameters.default_parameters()

    # Merge extra args into parameter dict
    if extra_ffc_args is not None:
        for key, value in extra_ffc_args.items():
            parameters[key] = value

    # For now, don't support lists of forms
    assert not isinstance(form, list)
    form_index = 0

    # Call FFC
    code_h, code_c = ffc.compiler.compile_form(form, prefix=prefix, parameters=parameters)

    # Build the concrete C function name of the generated function
    function_name = "tabulate_tensor_{}_cell_integral_{}_otherwise".format(prefix, form_index)
    # Find section of generated code that contains the tabulate_tensor function
    index_start = code_c.index("void {}(".format(function_name))
    index_end = code_c.index("ufc_cell_integral* create_{}_cell_integral_{}_otherwise(void)".format(prefix, form_index),
                             index_start)
    # Extract tabulate_tensor definition
    tabulate_tensor_code = code_c[index_start:index_end].strip()

    return function_name, tabulate_tensor_code


def compile_cffi(module_name: str,
                 code_c: str,
                 code_h: str,
                 compiler_args: List[str] = None,
                 verbose: bool = False):
    """Compiles a module from the specified source code using CFFI."""

    ffi = cffi.FFI()
    ffi.set_source(module_name, code_c, extra_compile_args=compiler_args)
    ffi.cdef(code_h)
    lib = ffi.compile(verbose=verbose)

    return lib


def import_cffi(module_name: str):
    """Imports a module generated by CFFI."""

    mod = importlib.import_module(module_name)
    ffi, lib = mod.ffi, mod.lib
    return ffi, lib


def gen_test_case() -> TestCase:
    """Generates an example test case."""

    # Default GCC parameters
    gcc_default = {
        "-O2": True,
        "-funroll-loops": False,
        "-ftree-vectorize": False,
        "-march=native": True,
        "-mtune=native": False,
    }

    # GCC parameters with auto vectorization
    gcc_auto_vectorize = {
        "-O2": True,
        "-funroll-loops": True,
        "-ftree-vectorize": True,
        "-march=native": True,
        "-mtune=native": True,
    }

    # Default FFC parameters
    ffc_default = {"optimize": True}
    # FFC with padding enabled
    ffc_padded = {"optimize": True, "padlen": 4}

    one_element = TestRunArgs(
        name="default",
        cross_element_width=0,
        ffc_args=ffc_default
    )

    one_element_padded = TestRunArgs(
        name="padded",
        cross_element_width=0,
        ffc_args=ffc_padded
    )

    four_elements = TestRunArgs(
        name="4x cross element",
        cross_element_width=4,
        ffc_args=ffc_default
    )

    # Combine all options: benchmark will be cartesian product of compiler_args, run_args and forms
    test_case = TestCase(
        compiler_args=[
            gcc_default,
            gcc_auto_vectorize
        ],
        run_args=[
            one_element,
            one_element_padded,
            four_elements
        ],
        forms=forms.get_all_forms(),
        reference_case=(0, 0),
        n_repeats=3
    )

    return test_case


def execute_test(test_case: TestCase) -> Dict[str, Dict[Tuple[int, int], FormTestResult]]:
    """Runs a TestCase and returns a performance report."""

    raw_results = {form_def.form_name: dict() for form_def in test_case.forms}

    # Step 1: Run benchmark
    # ---

    # Outermost loop over all compiler argument sets
    for i, compiler_arg_set in enumerate(test_case.compiler_args):
        active_compile_args = [arg for arg, use in compiler_arg_set.items() if use]

        test_names = {form_def.form_name: [] for form_def in test_case.forms}
        test_codes = []
        test_signatures = []

        # Step 1a: Code generation
        # ---

        # Loop over all run arguments sets (e.g. FFC arguments)
        for j, run_arg_set in enumerate(test_case.run_args):  # type: int, TestRunArgs
            ffc_arg_set = copy(run_arg_set.ffc_args)
            cross_element_width = run_arg_set.cross_element_width
            if cross_element_width > 0:
                ffc_arg_set["cross_element_width"] = cross_element_width
            else:
                cross_element_width = 1

            # Loop over all forms to generate tabulate_tensor code
            for form_def in test_case.forms:  # type: FormTestData
                # Generate the UFL form
                form = form_def.code_gen()

                # Run FFC (tabulate_tensor code generation)
                raw_function_name, raw_code = compile_form(form, form_def.form_name + "_" + str(j),
                                                           extra_ffc_args=ffc_arg_set)
                # Wrap tabulate_tensor code in test runner function
                test_name = "_test_runner" + raw_function_name
                code, signature = wrap_tabulate_tensor_code(test_name, raw_function_name, raw_code, form_def,
                                                            cross_element_width)

                # Store generated content
                test_names[form_def.form_name].append(test_name)
                test_codes.append(code)
                test_signatures.append(signature)

        # Step 1b: Code compilation
        # ---

        # Concatenate all test codes
        full_c, full_h = join_test_wrappers(test_codes, test_signatures)

        # Build the test module
        compile_cffi("_benchmark_{}".format(i), full_c, full_h, compiler_args=active_compile_args, verbose=True)
        ffi, lib = import_cffi("_benchmark_{}".format(i))

        # Step 1c: Run benchmark
        # ---

        # Loop over the forms
        for form_idx, (form_name, test_names) in enumerate(test_names.items()):
            # Loop over the function names of each test case
            for j, test_name in enumerate(test_names):
                run_arg_set = test_case.run_args[j]  # type: TestRunArgs
                form_data = test_case.forms[form_idx]  # type: FormTestData

                n_elem = form_data.n_elems
                w = form_data.coefficients
                coords_dof = form_data.coord_dofs

                # If cross element vectorization is enabled, tile coefficient and dof arrays accordingly
                cross_element_width = run_arg_set.cross_element_width
                if cross_element_width > 0:
                    n_elem = int(n_elem / cross_element_width)

                    w = np.tile(np.expand_dims(w, 2), cross_element_width)
                    coords_dof = np.tile(np.expand_dims(coords_dof, 2), cross_element_width)

                # Get pointers to numpy arrays
                w_ptr = ffi.cast("double*", w.ctypes.data)
                coords_dof_ptr = ffi.cast("double*", coords_dof.ctypes.data)

                # Get the function that should be called
                fun = getattr(lib, test_name)

                n_runs = test_case.n_repeats
                test_call = lambda: fun(n_elem, w_ptr, coords_dof_ptr)
                # Run the timing
                avg, min, max = utils.timing(n_runs, test_call, verbose=True)

                # Store result
                raw_results[form_name][(i, j)] = avg, min, max

    # Step 2: Generate report (calculate speedup)
    # ---

    # Dict for results that will be returned (includes speedup)
    results = {form_def.form_name: dict() for form_def in test_case.forms}

    # Loop over results of all forms
    for form_idx, (form_name, form_results) in enumerate(raw_results.items()):
        # Obtain the reference time for the form
        reference_time = form_results[(test_case.reference_case)][0]

        for i, compiler_arg_set in enumerate(test_case.compiler_args):
            for j, run_arg_set in enumerate(test_case.run_args):
                raw_result = form_results[(i, j)]
                speedup = reference_time / raw_result[0]

                # Store the result values including speedup
                results[form_name][(i, j)] = FormTestResult(*raw_result, speedup)

    return results


def print_report(test_case: TestCase, report):
    """Prints a report generated by 'execute_test'."""

    print("")
    print("Benchmark report")
    print("-" * 20)

    # Get the length of the longest run args name
    longest_name = 0
    for run_arg_set in test_case.run_args:  # type: TestRunArgs
        longest_name = np.maximum(longest_name, len(run_arg_set.name))

    # Loop over results of all forms
    for form_idx, (form_name, form_results) in enumerate(report.items()):
        print("{}".format(form_name))
        print("Results for form '{}', test run with n={} elements".format(form_name, test_case.forms[form_idx].n_elems))

        for i, compiler_arg_set in enumerate(test_case.compiler_args):
            active_compile_args = [arg for arg, use in compiler_arg_set.items() if use]
            print("\tCompiled with flags '{}'".format(', '.join(active_compile_args)))

            for j, run_arg_set in enumerate(test_case.run_args):
                result = form_results[(i, j)]  # type: FormTestResult

                print(
                    "\t\t{:<{name_length}} | avg: {:>8.2f}ms | min: {:>8.2f}ms | max: {:>8.2f}ms | speedup: {:>5.2f}x".format(
                        run_arg_set.name,
                        result.avg * 1000,
                        result.min * 1000,
                        result.max * 1000,
                        result.speedup,
                        name_length=longest_name + 1))

            print("")

        print("-" * 40)
        print("")


def run():
    test_case = gen_test_case()
    report = execute_test(test_case)
    print_report(test_case, report)
    return
