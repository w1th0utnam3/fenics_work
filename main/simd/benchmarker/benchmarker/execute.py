import cffi
import importlib
import numpy as np
from typing import Dict, List

from benchmarker.types import BenchmarkReport, TestCase, TestRunArgs, FormTestData, FormTestResult

import simd.utils as utils


def compile_cffi(module_name: str,
                 code_c: str,
                 code_h: str,
                 compiler_args: List[str] = None,
                 verbose: bool = False) -> str:
    """Compiles a module from the specified source code using CFFI."""

    ffi = cffi.FFI()
    ffi.set_source(module_name, code_c, extra_compile_args=compiler_args)
    ffi.cdef(code_h)
    lib = ffi.compile(verbose=verbose)

    return lib


def import_cffi(module_name: str):
    """Imports a module generated by CFFI."""

    mod = importlib.import_module(module_name)
    ffi, lib = mod.ffi, mod.lib
    return ffi, lib


def run_benchmark(test_case: TestCase, test_fun_names: Dict, code_c: str, code_h: str) -> BenchmarkReport:
    """
    Runs a benchmark TestCase and generates a corresponding BenchmarkReport.

    :param test_case: The TestCase to process.
    :param test_fun_names: A dict containing the C function names of the test wrappers, from generate_benchmark_code.
    :param code_c: The C code of the test case.
    :param code_h: The C header of the test case.
    :return: Benchmark report: dict over all form names, containing dicts (i,j) -> FormTestResult, where (i,j) are
        combinations of compiler parameters and form compiler parameters.
    """

    raw_results = {form_def.form_name: dict() for form_def in test_case.forms}

    # Step 1: Run benchmark
    # ---

    # Outermost loop over all compiler argument sets
    for i, compiler_arg_set in enumerate(test_case.compiler_args):
        active_compile_args = [arg for arg, use in compiler_arg_set.items() if use]

        # Step 1a: Code compilation
        # ---

        # Build the test module
        compile_cffi("_benchmark_{}".format(i), code_c, code_h, compiler_args=active_compile_args, verbose=True)
        ffi, lib = import_cffi("_benchmark_{}".format(i))

        # Step 1b: Run benchmark
        # ---

        # Loop over the forms
        for form_idx, (form_name, form_fun_names) in enumerate(test_fun_names.items()):
            # Loop over the function names of each test case
            for j, fun_name in enumerate(form_fun_names):
                run_arg_set = test_case.run_args[j]  # type: TestRunArgs
                form_data = test_case.forms[form_idx]  # type: FormTestData

                n_elem = form_data.n_elems
                w = form_data.coefficients
                coords_dof = form_data.coord_dofs

                # If cross element vectorization is enabled, tile coefficient and dof arrays accordingly
                cross_element_width = run_arg_set.cross_element_width
                if cross_element_width > 0:
                    n_elem = int(n_elem / cross_element_width)

                    w = np.tile(np.expand_dims(w, 2), cross_element_width)
                    coords_dof = np.tile(np.expand_dims(coords_dof, 2), cross_element_width)

                # Get pointers to numpy arrays
                w_ptr = ffi.cast("double*", w.ctypes.data)
                coords_dof_ptr = ffi.cast("double*", coords_dof.ctypes.data)

                # Get the function that should be called
                fun = getattr(lib, fun_name)

                # How many times the test should be run
                n_runs = test_case.n_repeats
                # The actual test callable
                test_callable = lambda: fun(n_elem, w_ptr, coords_dof_ptr)

                # Run the timing
                avg, min, max = utils.timing(n_runs, test_callable, verbose=True, name=fun_name)

                # Store result
                raw_results[form_name][(i, j)] = avg, min, max

    # Step 2: Generate report and calculate speedup)
    # ---

    # Dict for results that will be returned (includes speedup)
    results = {form_def.form_name: dict() for form_def in test_case.forms}

    # Loop over results of all forms
    for form_idx, (form_name, form_results) in enumerate(raw_results.items()):
        # Obtain the reference time for the form
        reference_time = form_results[(test_case.reference_case)][0]

        for i, compiler_arg_set in enumerate(test_case.compiler_args):
            for j, run_arg_set in enumerate(test_case.run_args):
                raw_result = form_results[(i, j)]
                speedup = reference_time / raw_result[0]

                # Store the result values including speedup
                results[form_name][(i, j)] = FormTestResult(*raw_result, speedup)

    return results
