import os
import json
import pickle
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from benchmarker.types import TestCase, BenchmarkReport, TestRunArgs, FormTestResult


def print_report(test_case: TestCase, report: BenchmarkReport):
    """Prints a report generated by execute_test."""

    print("")
    print("Benchmark report")
    print("-" * 20)
    print("Total runtime: {:.2f}s".format(report.total_runtime))

    # Get the length of the longest run args name
    longest_name = 0
    for run_arg_set in test_case.run_args:  # type: TestRunArgs
        longest_name = np.maximum(longest_name, len(run_arg_set.name))

    # Loop over results of all forms
    for form_idx, (form_name, form_results) in enumerate(report.results.items()):
        print("{}".format(form_name))
        print("Results for form '{}', test run with n={} elements".format(form_name, test_case.forms[form_idx].n_elems))

        for i, compiler_arg_set in enumerate(test_case.compiler_args):
            active_compile_args = [arg for arg, use in compiler_arg_set.items() if use]
            print("\tCompiled with flags '{}'".format(', '.join(active_compile_args)))

            for j, run_arg_set in enumerate(test_case.run_args):
                result = form_results[(i, j)]  # type: FormTestResult

                print(
                    "\t\t{:<{name_length}} | avg: {:>8.2f}ms | min: {:>8.2f}ms | max: {:>8.2f}ms | speedup: {:>5.2f}x | result: {:>20.14e} | result ok: {}".format(
                        run_arg_set.name,
                        result.avg * 1000,
                        result.min * 1000,
                        result.max * 1000,
                        result.speedup,
                        result.result_val,
                        result.result_ok,
                        name_length=longest_name + 1))

            print("")

        print("-" * 40)
        print("")


    pass
def plot_data(data: PlotData):
    # Column labels
    columns = ["{}, {}".format(data.compiler_args[i][j],
                               data.runtime_args[i][k]) for (i, j, k) in data.plot_indices]

    # Initialize data array
    speedup = np.zeros((len(data.results), len(columns)))

    # Collect speedup values
    for form_idx, (form_name, form_results) in enumerate(data.results.items()):
        reference_time = form_results[data.reference_case].avg

        for plot_index in data.plot_indices[form_idx]:
            avg_time = form_results[plot_index].avg
            speedup[form_idx, col_idx] = reference_time / avg_time


def plot_report(test_case: TestCase, report: BenchmarkReport,
                compiler_arg_names: List[str],
                visualize_combinations: List[Tuple[int, int]],
                reference_combination: Tuple[int, int]):
    """
    Plots a benchmark report.

    :param test_case: The TestCase that was used to produce the benchmark.
    :param report: The BenchmarkReport that should be plotted.
    :param compiler_arg_names: Names that should be displayed for the sets of compiler parameters.
    :param visualize_combinations: List of tuples with indices that indicate which compiler and runtime parameter pairs
        should be plotted
    :param reference: The reference case that should be used to calculate speedup.
    """

    form_names = [str(name) for name in report.results.keys()]
    run_args = [run_args.name for run_args in test_case.run_args]

    # Collect columns (visualized by one bar per form)
    columns = ["{}, {}".format(compiler_arg_names[i], run_args[j]) for (i, j) in visualize_combinations]

    speedup = np.zeros((len(report.results), len(columns)))

    # Calculate and collect speedup values
    for form_idx, (form_name, form_results) in enumerate(report.results.items()):
        reference_time = form_results[reference_combination].avg

        for k, (i, j) in enumerate(visualize_combinations):
            avg_time = form_results[(i, j)].avg
            speedup[form_idx, k] = reference_time / avg_time

    df = pd.DataFrame(speedup, columns=columns)
    # Assign form names to rows
    df = df.rename(index={i: name for i, name in enumerate(form_names)})
    # Create the plot
    df.plot.barh(figsize=(10, len(form_names)*1.2))

    ax = plt.gca()
    # Fix that horizontal bar plots are in wrong order
    ax.invert_yaxis()
    # Show x axis grid
    ax.grid(axis="x")

    plt.show()

    return


def save_generated_data(name: str, test_fun_names: Dict, codes: List[Tuple[str, str]], path: str = ""):
    """
    Stores the specified generated test case data in a set of files.

    :param name: The base name for the set files.
    :param path: Optional path where the files should be stored (otherwise uses working directory).
    """

    output_basename = os.path.join(path, name)

    def store_string(filename: str, content: str):
        with open(filename, mode="w") as f:
            f.write(content)

    test_fun_json = json.dumps(test_fun_names, indent=4)
    store_string(output_basename + "_funs.json", test_fun_json)

    for i, (code_c, code_h) in enumerate(codes):
        store_string(output_basename + "_code_{}.c".format(i), code_c)
        store_string(output_basename + "_code_{}.h".format(i), code_h)


def load_generated_data(name: str, path: str = "") -> Tuple[Dict, List[Tuple[str, str]]]:
    """
    Loads a set of generated test case data from files.

    :param name: The base name for the set of files as given to the save function.
    :param path: Optional path where the file are stored (otherwise uses working directory).
    :return: Tuple that can be used as input to run a benchmark.
    """

    input_basename = os.path.join(path, name)

    def load_string(filename: str) -> str:
        with open(filename, mode="r") as f:
            content = f.read()
            return content

    n_code_files = 0
    test_fun_json = load_string(input_basename + "_funs.json")
    test_fun_names = json.loads(test_fun_json)

    # Convert lists from json file to tuples
    for form_name, test_function_list in test_fun_names.items():
        for i in range(len(test_function_list)):
            fun_name, code_idx = test_function_list[i][0], test_function_list[i][1]
            test_function_list[i] = (fun_name, code_idx)

            n_code_files = max(n_code_files, code_idx)

    codes = []
    for i in range(n_code_files + 1):
        code_c = load_string(input_basename + "_code_{}.c".format(i))
        code_h = load_string(input_basename + "_code_{}.h".format(i))
        codes.append((code_c, code_h))

    return test_fun_names, codes


def save_report(filename: str, report: BenchmarkReport):
    """Stores a benchmark report to the specified file."""

    with open(filename, mode="wb") as f:
        pickle.dump(report, f)


def load_report(filename: str) -> BenchmarkReport:
    """Loads a benchmark report from the specified file."""

    with open(filename, mode="rb") as f:
        report = pickle.load(f)

    return report
